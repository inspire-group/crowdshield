{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sys\n",
    "PREPROCESS_PATH = '../lib/twitter_preprocess'\n",
    "sys.path.append(PREPROCESS_PATH)\n",
    "from twitter_preprocess import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path_25 = 'glove.twitter.27B/glove.twitter.27B.25d.txt'\n",
    "with open(glove_path_25, 'rb') as lines:\n",
    "    w2v_25 = {line.split()[0]: np.array(map(float, line.split()[1:])) for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path_100 = 'glove.twitter.27B/glove.twitter.27B.100d.txt'\n",
    "with open(glove_path_100, 'rb') as lines:\n",
    "    w2v_100 = {line.split()[0]: np.array(map(float, line.split()[1:])) for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path_200 = 'glove.twitter.27B/glove.twitter.27B.200d.txt'\n",
    "with open(glove_path_200, 'rb') as lines:\n",
    "    w2v_200 = {line.split()[0]: np.array(map(float, line.split()[1:])) for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(object):\n",
    "    \n",
    "    def __init__(self, train_path, test_path, context_path=None, word2vec=None):\n",
    "        pass\n",
    "        \n",
    "    def load_context(self, context_path):\n",
    "        file_path = context_path\n",
    "        delimiter = ';'\n",
    "        reader = csv.reader(open(file_path,\"rb\"),delimiter=delimiter)\n",
    "        texts = []\n",
    "        for row in reader:\n",
    "            text = row[4]\n",
    "            texts.append(text)\n",
    "        context = texts\n",
    "        return context\n",
    "        \n",
    "    def load_data(self, data_path):\n",
    "        file_path = data_path\n",
    "        delimiter = ';'\n",
    "        reader = csv.reader(open(file_path,\"rb\"),delimiter=delimiter)\n",
    "        rows = []\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "        labels  = np.array([int(x[0]) for x in rows])\n",
    "        data  = [x[1] for x in rows]\n",
    "        return data, labels\n",
    "        \n",
    "    def preprocess(self, data):\n",
    "        tweets = data\n",
    "        tokenizer = TweetTokenizer()\n",
    "        preproc_tweets = map(lambda t: tokenize(t), tweets)\n",
    "        preproc_tweets = map(lambda t: tokenizer.tokenize(t), preproc_tweets)\n",
    "        #preproc_tweets = map(lambda t: filter(lambda w: w not in stop_words, t), preproc_tweets)\n",
    "        return preproc_tweets\n",
    "    \n",
    "    def feature_extract(self, preproc_data):\n",
    "        tweets = preproc_data\n",
    "        word2vec = self.word2vec\n",
    "        mean_embeds = np.array([\n",
    "                        np.mean([word2vec[w] for w in words if w in word2vec]\n",
    "                                or [np.zeros(dim)], axis=0)\n",
    "                        for words in tweets\n",
    "                    ])\n",
    "        return mean_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecFeatureExtractor(FeatureExtractor):\n",
    "    def __init__(self, train_path, test_path, context_path, size):\n",
    "        context = self.load_context(context_path)\n",
    "        preproc_context = self.preprocess(context)\n",
    "        model = Word2Vec(preproc_context, size=size)\n",
    "        self.word2vec = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "        \n",
    "        train, train_labels = self.load_data(train_path)\n",
    "        preproc_train = self.preprocess(train)\n",
    "        self.train_vecs = self.feature_extract(preproc_train)\n",
    "        self.train_labels = train_labels\n",
    "        \n",
    "        test, test_labels = self.load_data(test_path)\n",
    "        preproc_test = self.preprocess(test)\n",
    "        self.test_vecs = self.feature_extract(preproc_test)\n",
    "        self.test_labels = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveFeatureExtractor(FeatureExtractor):\n",
    "    def __init__(self, train_path, test_path, context_path, glove):\n",
    "        _ = context_path\n",
    "        self.word2vec = glove\n",
    "        \n",
    "        train, train_labels = self.load_data(train_path)\n",
    "        preproc_train = self.preprocess(train)\n",
    "        self.train_vecs = self.feature_extract(preproc_train)\n",
    "        self.train_labels = train_labels\n",
    "        \n",
    "        test, test_labels = self.load_data(test_path)\n",
    "        preproc_test = self.preprocess(test)\n",
    "        self.test_vecs = self.feature_extract(preproc_test)\n",
    "        self.test_labels = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of\n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    \n",
    "    def __init__(self, fe):\n",
    "        self.train_vecs = fe.train_vecs\n",
    "        self.train_labels = fe.train_labels\n",
    "        self.test_vecs = fe.test_vecs\n",
    "        self.test_labels = fe.test_labels\n",
    "        self.clf = None\n",
    "        \n",
    "    def sample_data(self, sample_size):\n",
    "        if not sample_size:\n",
    "            return\n",
    "        sample_idxs = np.random.choice(self.train_vecs.shape[0], sample_size, False)\n",
    "        self.train_vecs = self.train_vecs[sample_idxs]\n",
    "        self.train_labels = self.train_labels[sample_idxs]\n",
    "    \n",
    "    def fit_predict(self):\n",
    "        train_vecs = self.train_vecs\n",
    "        train_labels = self.train_labels\n",
    "        test_vecs = self.test_vecs\n",
    "        clf = self.clf\n",
    "        clf.fit(train_vecs, train_labels)\n",
    "        self.preds = clf.predict(test_vecs)\n",
    "        \n",
    "    def evaluate(self):\n",
    "        test_labels = self.test_labels\n",
    "        preds = self.preds\n",
    "        assert(len(test_labels)==len(preds)), \"Prediction length differs from test lengths\"\n",
    "        y = test_labels\n",
    "        tp, fp, tn, fn = 0, 0, 0, 0\n",
    "        for i in range(len(preds)):\n",
    "            if preds[i] == 1 and y[i] == 1: tp += 1\n",
    "            elif preds[i] == 1 and y[i] == 0: fp += 1\n",
    "            elif preds[i] == 0 and y[i] == 0: tn += 1\n",
    "            else: fn += 1\n",
    "        precision = tp / float(tp + fp) if (tp + fp) > 0 else float(tp)\n",
    "        recall = tp / float(tp + fn) if (tp + fn) > 0 else float(tp)\n",
    "        f1 = 2 / (1/precision + 1/recall) if precision > 0 and recall > 0 else 0\n",
    "        #print \"Precision: {}, Recall: {}, F1: {}\".format(precision, recall, f1)\n",
    "        return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier(Classifier):\n",
    "    \n",
    "    def __init__(self, fe, C=0.3):\n",
    "        self.train_vecs = fe.train_vecs\n",
    "        self.train_labels = fe.train_labels\n",
    "        self.test_vecs = fe.test_vecs\n",
    "        self.test_labels = fe.test_labels\n",
    "        self.clf = svm.SVC(C=0.3, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_routine(context_path, train_path, test_path, sample_size=None, iters=1):\n",
    "    \n",
    "    dim25, dim100, dim200 = [], [], []\n",
    "    \n",
    "    #print \"################# GLOVE 25-DIM ##################\"\n",
    "    w2v_25 = w2v\n",
    "    fe25 = GloveFeatureExtractor(train_path, test_path, None, w2v_25)\n",
    "\n",
    "    #print \"################# GLOVE 100-DIM ##################\"\n",
    "    fe100 = GloveFeatureExtractor(train_path, test_path, None, w2v_100)\n",
    "    \n",
    "    #print \"################# GLOVE 200-DIM ##################\"\n",
    "    fe200 = GloveFeatureExtractor(train_path, test_path, None, w2v_200)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        clf25 = SVMClassifier(fe25, C=0.3)\n",
    "        clf25.sample_data(sample_size)\n",
    "        clf25.fit_predict()\n",
    "        p, r, f1 = clf25.evaluate()\n",
    "        dim25.append((p, r, f1))\n",
    "        \n",
    "        clf100 = SVMClassifier(fe100, C=0.3)\n",
    "        clf100.sample_data(sample_size)\n",
    "        clf100.fit_predict()\n",
    "        p, r, f1 = clf100.evaluate()\n",
    "        dim100.append((p, r, f1))\n",
    "        \n",
    "        clf200 = SVMClassifier(fe200, C=0.3)\n",
    "        clf200.sample_data(sample_size)\n",
    "        clf200.fit_predict()\n",
    "        p, r, f1 = clf200.evaluate()\n",
    "        dim200.append((p, r, f1))\n",
    "    \n",
    "    return dim25, dim100, dim200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_routine(context_path, train_path, test_path, sample_size=None, iters=1):\n",
    "    dim25, dim100, dim200 = [], [], []\n",
    "    \n",
    "    #print \"################# W2V 25-DIM ##################\"\n",
    "    fe25 = Word2VecFeatureExtractor(train_path, test_path, context_path, size=25)\n",
    "\n",
    "    #print \"################# W2V 100-DIM ##################\"\n",
    "    fe100 = Word2VecFeatureExtractor(train_path, test_path, context_path, size=100)\n",
    "    \n",
    "    #print \"################# W2V 200-DIM ##################\"\n",
    "    fe200 = Word2VecFeatureExtractor(train_path, test_path, context_path, size=200)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        clf25 = SVMClassifier(fe25, C=0.3)\n",
    "        clf25.sample_data(sample_size)\n",
    "        clf25.fit_predict()\n",
    "        p, r, f1 = clf25.evaluate()\n",
    "        dim25.append((p, r, f1))\n",
    "        \n",
    "        clf100 = SVMClassifier(fe100, C=0.3)\n",
    "        clf100.sample_data(sample_size)\n",
    "        clf100.fit_predict()\n",
    "        p, r, f1 = clf100.evaluate()\n",
    "        dim100.append((p, r, f1))\n",
    "        \n",
    "        clf200 = SVMClassifier(fe200, C=0.3)\n",
    "        clf200.sample_data(sample_size)\n",
    "        clf200.fit_predict()\n",
    "        p, r, f1 = clf200.evaluate()\n",
    "        dim200.append((p, r, f1))\n",
    "    \n",
    "    return dim25, dim100, dim200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_path = 'data/KaepernickStorm.csv'\n",
    "train_path = 'data/KaepernickTrain.csv'\n",
    "test_path = 'data/KaepernickTest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0.6527415143603134, 0.8865248226950354, 0.7518796992481203)],\n",
       " [(0.7781350482315113, 0.8581560283687943, 0.8161888701517708)],\n",
       " [(0.7993421052631579, 0.8617021276595744, 0.8293515358361774)])"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_routine(context_path, train_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0.7609427609427609, 0.8014184397163121, 0.7806563039723662)],\n",
       " [(0.7792642140468228, 0.8262411347517731, 0.802065404475043)],\n",
       " [(0.7766666666666666, 0.8262411347517731, 0.8006872852233676)])"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_routine(context_path, train_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim25, dim100, dim200 = glove_routine(context_path, train_path, test_path, sample_size=30, iters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.64145413  0.83922902  0.70336741]\n",
      "[ 0.14557557  0.39825438  0.14454008]\n",
      "\n",
      "[ 0.66215591  0.8537415   0.71241291]\n",
      "[ 0.18644099  0.4483573   0.24083377]\n",
      "\n",
      "[ 0.64997976  0.92040816  0.75120677]\n",
      "[ 0.1257234   0.2811678   0.12848133]\n"
     ]
    }
   ],
   "source": [
    "#print scipy.stats.hmean(dim25, axis=0)\n",
    "print np.mean(dim25, axis=0)\n",
    "print np.std(dim25, axis=0) * 2\n",
    "\n",
    "print\n",
    "#print scipy.stats.hmean(dim100, axis=0)\n",
    "print np.mean(dim100, axis=0)\n",
    "print np.std(dim100, axis=0) * 2\n",
    "\n",
    "print\n",
    "#print scipy.stats.hmean(dim200, axis=0)\n",
    "print np.mean(dim200, axis=0)\n",
    "print np.std(dim200, axis=0) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim25, dim100, dim200 = word2vec_routine(context_path, train_path, test_path, sample_size=30, iters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.70722512  0.88798186  0.7781283 ]\n",
      "[ 0.13115082  0.2320046   0.07951363]\n",
      "\n",
      "[ 0.68789257  0.9154195   0.77451519]\n",
      "[ 0.11958691  0.27384217  0.1119187 ]\n",
      "\n",
      "[ 0.6843087   0.90929705  0.77448646]\n",
      "[ 0.11809007  0.18948017  0.06013639]\n"
     ]
    }
   ],
   "source": [
    "print np.mean(dim25, axis=0)\n",
    "print np.std(dim25, axis=0) * 2\n",
    "\n",
    "print\n",
    "print np.mean(dim100, axis=0)\n",
    "print np.std(dim100, axis=0) * 2\n",
    "\n",
    "print\n",
    "print np.mean(dim200, axis=0)\n",
    "print np.std(dim200, axis=0) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7005649717514124, 0.8794326241134752, 0.779874213836478), (0.6518324607329843, 0.8829787234042553, 0.7499999999999999), (0.6597938144329897, 0.45390070921985815, 0.5378151260504201), (0.7589928057553957, 0.74822695035461, 0.7535714285714286), (0.6150341685649203, 0.9574468085106383, 0.7489597780859918)]\n"
     ]
    }
   ],
   "source": [
    "print dim200[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_path = 'data/MelaniaTrumpStorm.csv'\n",
    "train_path = 'data/MelaniaTrumpTrain.csv'\n",
    "test_path = 'data/MelaniaTrumpTest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
